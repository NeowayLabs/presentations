Real Life Kubernetes
Neoway Study Case
12 May 2016
Tags: DevOps, k8s, microservices, cluster orchestration

Fabio Favero Henkes
Lead Developer - Prevention Team, Neoway
fabio.favero@neoway.com.br
http://www.neoway.com.br
@ffhenkes

Tiago CÃ©sar Katcipis
Metaphores enthusiast / Bug Factory - Datapirates Team, Neoway
tiago.katcipis@neoway.com.br
https://github.com/katcipis
@tiagokatcipis


* First, a little about Neoway

- Fabio talks awesome stuff about Neoway Business :-)


* Why should you care ?


* Microservices !!!

.image ./img/look-the-languages.jpg _ 700


* Microservices: The tradeoff

.image ./img/deploying-all-languages.jpg _ 700


* Microservices: For each one

- Deploy
- Service discovery
- Load balance
- Health check
- Scale
- Get the logs


* Development <-> Production distance

.image ./img/code-prod.jpg _ 500


* Cloud ephemeral infrastructure

.image ./img/instances-down.jpg _ 550


* Multi-tenancy

.image ./img/multi-tenancy.jpg _ 650


* The solution ?

.image ./img/automate.jpg _ 700


* What we wanted ?

- A way to automate everything
- As little distance as possible between running on the dev machine and production
- Easy way to: configure, scale, service discovery, load balance, health check, rolling update, debug
- Adding new services must be easy (we are going to have a lot of them :-)
- No need to ssh/log into nodes (this should be an exception, not the norm)
- No snowflakes servers
- Easy way to enable multi-tenancy (some teams have this challenge at Neoway)


* The Deployable artifact: Docker

- Self-contained (no hell of dependencies)
- Language agnostic way to package and run applications
- Isolated and automated way to build development environments
- Closes the gap between environments (dev <-> staging <-> production)
- Solves pretty nicely the artifact problem
- But how to run 100 containers through 10 machines ?
- How to connect different containers with each other ?
- How to keep then running even on failures ?


* The Container Orchestrator: Kubernetes

- Scheduling
- Scaling
- Load Balance
- Health Check
- Service discovery


* Kubernetes why ?

- More than just using fleet
- Less than using Mesos/Mesosphere/Deis
- We wanted something more, but that doesn't was a PAAS (remember, multi-tenancy)
- Kubernetes is a CAAS (Container As A Service)
- Just what we wanted :-)
- Cool / Scalable way to extend through REST API
- Great community (Google + Red Hat as top contributors, not bad :-)
- Great documentation


* Datapirates Kubernetes Learning Experience


* Datapirates ???

- Team responsible for capturing data on the internet
- We mainly develop scrapers
- And all the infrastructure required by it
- Breaking captchas
- Fresh proxies to avoid blacklisting
- Storing raw data (rastreability / reprocessing)
- Monitor services + more than 200 bots
- DevOps culture, the team does all operational work


* Context: April 2015 - Current architecture

- Java framework performs crawler
- All bots share the same java JVM (installed on host)
- We have very interesting problems when we try to update Java :-)
- There is no isolation or orthogonality


* Context: April 2015 - New architecture

- Started work on new service oriented architecture for bots
- Looking for a way to run our new architecture
- Strong desire to have better isolation and orthogonality for the bots
- We started working with Kubernetes for this around August, 2015
- Setting up a cluster took less than a day (with help of a kick ass BlackOps/DevOps/PaleoOps)
- Understand the main concepts is pretty fast
- Understanding the networking took almost a week and a lot of drawings on the wall :-)


* Kubernetes concepts very fast introduction


* Pods

- Where your containers will be running
- Each one has its own IP address
- Can have multiple containers
- Containers on the same pod share the same filesystem/network/IPC(Inter process communication)
- You can think on it as a "node"
- If you create a pod and it exits, it stays dead forever
- Enters replication controllers


* Replication controllers

- Replicates your pods
- Keep them running, if a pod dies it starts a new one
- On Kubernetes 1.2 there is the new Replication Set, which is recommended
- We are still working with replication controllers
- Neither pods or replication controllers provides service discovery
- Until now, you have not exported anything
- Enters Services


* Services

- Easy way to do service discovery inside the cluster
- Creates an Cluster IP (more on that later :-) for your service
- The Cluster IP NEVER changes, but pods IPs changes, since they can exit
- Consumers always uses this cluster ip
- This Cluster IP load balances to a set of pods
- The set of pods is defined by the Selector concept
- It will hold a set of Endpoints (each endpoint points to a pod)
- They can also be used to abstract access to services outside the cluster


* Service discovery

- Environment variables
- DNS


* What we needed to deploy

.image ./img/datapirates-arch.png 580 _


* Our full stack

- CoreOS
- Etcd
- Flannel (SDN-Software Defined Network, or overlay network)
- Kubernetes
- [[https://github.com/NeowayLabs/cloud-machine][Cloud Machine]]


* How the cluster is builded ?

- In house tool called harbor, using Cloud Machine
- Why not [[https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html][kube-aws]] ?
- Because it didnt existed on the time we started :-)
- It seems to be a good way to start playing with Kubernetes + CoreOS


* Deployment Overview


* Where each service/database is

.image ./img/deployment.png _ 750


* Networking Example


* How a bot reaches a service ?

.image ./img/kubernetes-networking.png


* Basically ...

.image ./img/magic.gif _ 600


* Datapirates final remarks

- Choose a service discovery that works the same on dev/prod
- Do not use docker link env vars, they are deprecating on Docker 1.10
- DNS seems the way to go
- Being able to build entire cluster is 5 minutes was a good way to experiment with different instance types
- No need to knows IPs (or have some cheatsheet)
- Destroying stuff is fun :-)
- Easy to get logs and get inside containers
- Hard to debug network problems (more layers than usual)


* Overview

.image ./img/kubernetes.png


* Pathfinder

- Concept
- Architecture
- Deploy
- AWS

* Pathfinder Architecture Overview

.image ./img/overview.png 490 581

* Pathfinder AWS Infrastructure

.image ./img/networking.png 667 811

* Pathfinder Autos Scaling

.image ./img/autoscaling.png


* Interesting reading

- [[https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/admin/networking.md][Kubernetes Networking]]
